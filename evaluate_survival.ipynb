{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate a model from its best checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import wandb\n",
                "\n",
                "from model.metrics import Metrics\n",
                "from model.main import CNN\n",
                "from utils.helpers import get_config\n",
                "\n",
                "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Set the config file to train the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Specify the model to evaluate by passing the corresponding config file from the configs/ directory\n",
                "CONFIG = \"configs/sparsebagnet_cox.yml\"\n",
                "LOG_WANDB = True\n",
                "\n",
                "# Set the split(s) to evaluate on (a list of \"val\" and/or \"test\")\n",
                "splits = [\"test\"] "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Run evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "c = get_config(CONFIG)\n",
                "PROJECT = c.cnn.project\n",
                "RUN_ID = c.cnn.run_id\n",
                "\n",
                "# Evaluate from best checkpoint\n",
                "c.cnn.load_best_model = True\n",
                "c.cnn.resume_training = False\n",
                "\n",
                "estimator = Metrics(c)\n",
                "cnn = CNN(c=c, estimator=estimator)\n",
                "\n",
                "# Ignore the config entry \"eval_sets\" and evaluate on the splits defined above\n",
                "for split in splits:\n",
                "    print(\"\\nEvaluating on: \", split)\n",
                "\n",
                "    cnn.evaluate(split=split, log_all = LOG_WANDB)\n",
                "   \n",
                "    print(cnn.estimator.get_all_performances())\n",
                "\n",
                "# Finish the run, otherwise it might log into the last run when re-running the script\n",
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
